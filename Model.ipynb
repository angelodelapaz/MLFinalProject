{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commonted out the below lines as the packages are already installed in the environment\n",
    "\n",
    "# !pip install timm\n",
    "# !pip install nibabel\n",
    "# !pip install nilearn\n",
    "# !pip install light-the-torch && ltt install torch\n",
    "# !pip install torchio\n",
    "\n",
    "import tensorflow as tf\n",
    "#for neuroimaging data\n",
    "import nibabel as nib\n",
    "import nilearn as nilearn\n",
    "import timm as timm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch \n",
    "import torchio as tio\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if CUDA is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU for inference\n"
     ]
    }
   ],
   "source": [
    "# Set the device to GPU if available, else CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using {torch.cuda.get_device_name()} for inference' if torch.cuda.is_available() else 'Using CPU for inference')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Images in Dataset: 26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "root_dir = 'brown'\n",
    "\n",
    "# List all subdirectories in the root directory\n",
    "subdirectories = [x[0] for x in os.walk(root_dir)]\n",
    "\n",
    "dataset = []\n",
    "\n",
    "# Iterate over the subdirectories\n",
    "for subdir in subdirectories:\n",
    "    # Check if the subdirectory contains 'anat' in its path\n",
    "    if 'anat' in subdir:\n",
    "        # List all files in the subdirectory\n",
    "        files = os.listdir(subdir)\n",
    "        # Iterate over the files\n",
    "        for file in files:\n",
    "            # Check if the file is a .nii.gz file\n",
    "            if file.endswith('.nii.gz'):\n",
    "                file_path = os.path.join(subdir, file)\n",
    "                # Load the Nifti image using TorchIO\n",
    "                subject = tio.Subject(\n",
    "                    mri=tio.ScalarImage(file_path)\n",
    "                )\n",
    "                dataset.append(subject)\n",
    "\n",
    "# Print the number of images in the dataset\n",
    "print('Number of Images in Dataset: ' + str(len(dataset)))\n",
    "\n",
    "# # Load NIfTI image using TorchIO\n",
    "# image_path = 'path/to/your/nifti/image.nii.gz'\n",
    "# subject = tio.Subject(\n",
    "#     mri=tio.ScalarImage(image_path)\n",
    "# )\n",
    "\n",
    "# # Transform the image (e.g., resample, normalize)\n",
    "# transform = tio.Compose([\n",
    "#     tio.Resample(4),  \n",
    "#     tio.ZNormalization(),\n",
    "# ])\n",
    "# transformed_subject = transform(subject)\n",
    "\n",
    "# # Convert to PyTorch tensor\n",
    "# tensor = transformed_subject.mri.data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import nibabel as nib\n",
    "\n",
    "# root_dir = 'brown'\n",
    "\n",
    "# # List all subdirectories in the root directory\n",
    "# subdirectories = [x[0] for x in os.walk(root_dir)]\n",
    "\n",
    "# dataset = []\n",
    "\n",
    "# # Iterate over the subdirectories\n",
    "# for subdir in subdirectories:\n",
    "#     # Check if the subdirectory contains 'anat' in its path\n",
    "#     if 'anat' in subdir:\n",
    "#         # List all files in the subdirectory\n",
    "#         files = os.listdir(subdir)\n",
    "#         # Iterate over the files\n",
    "#         for file in files:\n",
    "#             # Check if the file is a .nii.gz file\n",
    "#             if file.endswith('.nii.gz'):\n",
    "#                 file_path = os.path.join(subdir, file)\n",
    "#                 # Load the Nifti image\n",
    "#                 data = nib.load(file_path)\n",
    "#                 # Append the image data to the dataset\n",
    "#                 dataset.append(data)\n",
    "         \n",
    "# # Print the number of images in the dataset\n",
    "# print('Number of Images in Dataset: ' + str(len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subject' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subject' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "dataset[0].mri.data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Subject' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Access data array and metadata using torchio\u001b[39;00m\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m      4\u001b[0m affine \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39maffine\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData Shape: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Subject' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "# # Access data array and metadata\n",
    "# data = dataset[22].get_fdata()\n",
    "# affine = dataset[22].affine \n",
    "# header = dataset[22].header\n",
    "\n",
    "# # Display basic information\n",
    "# print(f'Image dimensions: {data.shape}')\n",
    "# print(f'Voxel dimensions: {header.get_zooms()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Display the image\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(\u001b[43mdata\u001b[49m[:, :, \u001b[38;5;241m90\u001b[39m], cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgray\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39maxis(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the image\n",
    "\n",
    "plt.imshow(data[:, :, 90], cmap='gray')\n",
    "plt.axis('on')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
      "/var/folders/hr/f5tx3_6x0qj4kc28vj9rp1hm0000gn/T/ipykernel_2156/4211580262.py:11: UserWarning: Casting data from int16 to float32\n",
      "  resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m resampled_dataset \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset:\n\u001b[0;32m---> 11\u001b[0m     resampled_data \u001b[38;5;241m=\u001b[39m \u001b[43mresample_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_affine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_voxel_dimensions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     resampled_dataset\u001b[38;5;241m.\u001b[39mappend(resampled_data)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Display the resampled image\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nilearn/image/resampling.py:664\u001b[0m, in \u001b[0;36mresample_img\u001b[0;34m(img, target_affine, target_shape, interpolation, copy, order, clip, fill_value, force_resample)\u001b[0m\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;66;03m# Iterate over a set of 3D volumes, as the interpolation problem is\u001b[39;00m\n\u001b[1;32m    661\u001b[0m     \u001b[38;5;66;03m# separable in the extra dimensions. This reduces the\u001b[39;00m\n\u001b[1;32m    662\u001b[0m     \u001b[38;5;66;03m# computational cost\u001b[39;00m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ind \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndindex(\u001b[38;5;241m*\u001b[39mother_shape):\n\u001b[0;32m--> 664\u001b[0m         \u001b[43m_resample_one_img\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m            \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m            \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m            \u001b[49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresampled_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mall_img\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_img_is_string\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip:\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;66;03m# force resampled data to have a range contained in the original data\u001b[39;00m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;66;03m# preventing ringing artefact\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;66;03m# We need to add zero as a value considered for clipping, as it\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;66;03m# appears in padding images.\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     vmin \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(np\u001b[38;5;241m.\u001b[39mnanmin(data), \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/nilearn/image/resampling.py:306\u001b[0m, in \u001b[0;36m_resample_one_img\u001b[0;34m(data, A, b, target_shape, interpolation_order, out, copy, fill_value)\u001b[0m\n\u001b[1;32m    302\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*has changed in SciPy 0.18.*\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;66;03m# The resampling itself\u001b[39;00m\n\u001b[0;32m--> 306\u001b[0m     \u001b[43maffine_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolation_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_not_finite:\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;66;03m# Suppresses warnings in https://github.com/nilearn/nilearn/issues/1363\u001b[39;00m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/scipy/ndimage/_interpolation.py:625\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m matrix\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    619\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    620\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe behavior of affine_transform with a 1-D \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    621\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray supplied for the matrix parameter has changed in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    622\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSciPy 0.18.0.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    623\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    624\u001b[0m     )\n\u001b[0;32m--> 625\u001b[0m     \u001b[43m_nd_image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzoom_shift\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mmatrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     _nd_image\u001b[38;5;241m.\u001b[39mgeometric_transform(filtered, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, matrix, offset,\n\u001b[1;32m    629\u001b[0m                                   output, order, mode, cval, npad, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    630\u001b[0m                                   \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Resample the dataset\n",
    "\n",
    "from nilearn.image import resample_img\n",
    "\n",
    "# Define the target voxel dimensions\n",
    "target_voxel_dimensions = (1.0, 1.0, 1.0)\n",
    "\n",
    "# Resample the dataset to the target voxel dimensions\n",
    "resampled_dataset = []\n",
    "for data in dataset:\n",
    "    resampled_data = resample_img(data, target_affine=np.diag(target_voxel_dimensions))\n",
    "    resampled_dataset.append(resampled_data)\n",
    "\n",
    "# Display the resampled image\n",
    "resampled_data = resampled_dataset[22].get_fdata()\n",
    "plt.imshow(resampled_data[:, :, 90], cmap='gray')\n",
    "plt.axis('on')\n",
    "\n",
    "# Load the labels\n",
    "labels = pd.read_csv('brown/participants.tsv')\n",
    "labels.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Pre-Trained ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VideoResNet(\n",
       "  (stem): BasicStem(\n",
       "    (0): Conv3d(3, 64, kernel_size=(3, 7, 7), stride=(1, 2, 2), padding=(1, 3, 3), bias=False)\n",
       "    (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(64, 128, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(128, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(256, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv3DSimple(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
       "        (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.video import r3d_18\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = r3d_18(pretrained=True)\n",
    "\n",
    "# Modify the last layer\n",
    "num_classes = 2  # replace with the number of classes in your dataset\n",
    "model.fc = torch.nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Print the model\n",
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
